{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e34b9d77",
   "metadata": {},
   "source": [
    "# Text Analytics\n",
    "  1. Extract Sample document and apply following document preprocessing methods: Tokenization, POS Tagging, \n",
    "     stop words removal, Stemming and Lemmatization.\n",
    "  2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "     Frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b77ac21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fcaa7540",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "corpus = \"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome. The sky is pinkish-blue. You shouldn't eat cardboard\"\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062bb838",
   "metadata": {},
   "source": [
    "# Task1:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9384bb6d",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "719edc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8975bea",
   "metadata": {},
   "source": [
    "## Sentence Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cbbc30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and city is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard\"]\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = sent_tokenize(corpus)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5994a8c",
   "metadata": {},
   "source": [
    "## Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c933351d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "tokenized_word = word_tokenize(corpus)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558f1bf",
   "metadata": {},
   "source": [
    "## Stopwords Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc234904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7289782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'himself', 'can', \"shouldn't\", 'if', 'its', 'am', 'so', 'why', \"it's\", 'just', 'from', 'then', 'some', 'a', 'o', 'how', 'through', 'hadn', 'or', 'now', 'where', 'are', 'when', 'nor', \"you're\", 'off', 'few', 'doing', 'any', 'which', 'not', 'does', 'won', 'haven', 'have', 'll', \"mightn't\", 'for', 'until', \"hasn't\", 'being', 'me', 'themselves', 'shouldn', 'what', 'ourselves', \"mustn't\", 'she', 'down', 'out', 'y', \"haven't\", 'is', 'herself', 'but', 'above', 'only', 'between', 'here', 'her', 'under', 'most', 'about', 'own', 's', 'm', 'against', \"shan't\", 'don', 'no', 'further', \"you'd\", 'mightn', 'our', 'do', 'did', 'both', 'ain', \"you've\", 'isn', 'hers', 'yourselves', 'd', \"hadn't\", 'needn', 'whom', 'up', \"didn't\", 'other', 'as', \"that'll\", 'such', 'to', 'been', 'shan', 'was', 'below', 'these', 'with', 'very', 'wasn', \"she's\", 've', 'wouldn', 'that', 'their', 'over', \"won't\", 'during', 'itself', 'and', 'too', 'having', 'didn', 't', 'again', 'more', 'into', 'were', 'ma', 'mustn', 'by', 'all', 'yours', 'ours', 'you', 'myself', 'an', 'than', 'it', 'while', 'they', 'i', 'because', 'weren', 'will', 'those', 'before', 'in', 'on', 'your', 'this', 'has', 'them', 'yourself', 're', \"don't\", 'at', \"doesn't\", \"weren't\", 'same', 'after', \"aren't\", 'he', 'each', \"isn't\", \"needn't\", 'we', \"couldn't\", 'doesn', 'had', 'aren', 'there', 'be', 'couldn', 'theirs', \"should've\", 'him', 'my', 'the', \"wouldn't\", 'hasn', 'of', 'his', \"you'll\", 'once', 'should', 'who', \"wasn't\"}\n"
     ]
    }
   ],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85df8357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words: ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'city', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard']\n",
      "Filterd Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "filtered_sent = []\n",
    "for w in tokenized_word:\n",
    "    if w not in stop_words:\n",
    "        filtered_sent.append(w)\n",
    "print(\"Tokenized words:\",tokenized_word)\n",
    "print(\"Filterd Sentence:\",filtered_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f7bb47",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebb98dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fb686221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Stemmed Sentence: ['hello', 'mr.', 'smith', ',', 'today', '?', 'the', 'weather', 'great', ',', 'citi', 'awesom', '.', 'the', 'sky', 'pinkish-blu', '.', 'you', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "stemmed_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    stemmed_words.append(ps.stem(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc4f7b",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faeb8b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "059f6ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n",
      "Stemmed Sentence: ['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'city', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard']\n"
     ]
    }
   ],
   "source": [
    "lem = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for w in filtered_sent:\n",
    "    lemmatized_words.append(lem.lemmatize(w))\n",
    "\n",
    "print(\"Filtered Sentence:\",filtered_sent)\n",
    "print(\"Stemmed Sentence:\",lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eeaa4d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8449c989",
   "metadata": {},
   "source": [
    "## POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5897bc3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello', 'NNP'),\n",
       " ('Mr.', 'NNP'),\n",
       " ('Smith', 'NNP'),\n",
       " (',', ','),\n",
       " ('how', 'WRB'),\n",
       " ('are', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('doing', 'VBG'),\n",
       " ('today', 'NN'),\n",
       " ('?', '.'),\n",
       " ('The', 'DT'),\n",
       " ('weather', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('great', 'JJ'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('city', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('awesome', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('sky', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('pinkish-blue', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('You', 'PRP'),\n",
       " ('should', 'MD'),\n",
       " (\"n't\", 'RB'),\n",
       " ('eat', 'VB'),\n",
       " ('cardboard', 'NN')]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokenized_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb397ab",
   "metadata": {},
   "source": [
    "# Task2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afbcfd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data', 'science', 'the', 'for', 'machine', 'key', 'of', 'century', 'hardest', 'job', 'Science', 'data', 'is', 'learning', '21st'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"Data Science is the hardest job of the 21st century\"\n",
    "second_sentence = \"machine learning is the key for data science\"\n",
    "\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")\n",
    "\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b75f5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data': 1, 'science': 0, 'the': 2, 'for': 0, 'machine': 0, 'key': 0, 'of': 1, 'century': 1, 'hardest': 1, 'job': 1, 'Science': 1, 'data': 0, 'is': 1, 'learning': 0, '21st': 1}\n",
      "{'Data': 0, 'science': 1, 'the': 1, 'for': 1, 'machine': 1, 'key': 1, 'of': 0, 'century': 0, 'hardest': 0, 'job': 0, 'Science': 0, 'data': 1, 'is': 1, 'learning': 1, '21st': 0}\n"
     ]
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "    \n",
    "print(wordDictA)\n",
    "print(wordDictB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4ee0fb55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>science</th>\n",
       "      <th>the</th>\n",
       "      <th>for</th>\n",
       "      <th>machine</th>\n",
       "      <th>key</th>\n",
       "      <th>of</th>\n",
       "      <th>century</th>\n",
       "      <th>hardest</th>\n",
       "      <th>job</th>\n",
       "      <th>Science</th>\n",
       "      <th>data</th>\n",
       "      <th>is</th>\n",
       "      <th>learning</th>\n",
       "      <th>21st</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Data  science  the  for  machine  key  of  century  hardest  job  Science  \\\n",
       "0     1        0    2    0        0    0   1        1        1    1        1   \n",
       "1     0        1    1    1        1    1   0        0        0    0        0   \n",
       "\n",
       "   data  is  learning  21st  \n",
       "0     0   1         0     1  \n",
       "1     1   1         1     0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbf6275",
   "metadata": {},
   "source": [
    "# TF (Term Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "17a3463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6313b905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Data  science    the    for  machine    key   of  century  hardest  job  \\\n",
      "0   0.1    0.000  0.200  0.000    0.000  0.000  0.1      0.1      0.1  0.1   \n",
      "1   0.0    0.125  0.125  0.125    0.125  0.125  0.0      0.0      0.0  0.0   \n",
      "\n",
      "   Science   data     is  learning  21st  \n",
      "0      0.1  0.000  0.100     0.000   0.1  \n",
      "1      0.0  0.125  0.125     0.125   0.0  \n"
     ]
    }
   ],
   "source": [
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "print(tf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e60f8d",
   "metadata": {},
   "source": [
    "# IDF (Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e243323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    \n",
    "    for word, val in idfDict.items():\n",
    "        cnt = 0\n",
    "        for doc in docList:\n",
    "            if(doc[word] != 0):\n",
    "                cnt += 1\n",
    "        idfDict[word] = cnt\n",
    "#     print(idfDict)\n",
    "        \n",
    "        \n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) ))\n",
    "        \n",
    "    return(idfDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e6d6220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Data': 0.3010299956639812, 'science': 0.3010299956639812, 'the': 0.0, 'for': 0.3010299956639812, 'machine': 0.3010299956639812, 'key': 0.3010299956639812, 'of': 0.3010299956639812, 'century': 0.3010299956639812, 'hardest': 0.3010299956639812, 'job': 0.3010299956639812, 'Science': 0.3010299956639812, 'data': 0.3010299956639812, 'is': 0.0, 'learning': 0.3010299956639812, '21st': 0.3010299956639812}\n"
     ]
    }
   ],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB])\n",
    "print(idfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b0f886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0857151f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
